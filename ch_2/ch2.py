# 目标：实现一个采样和数据加载策略，将token编码为 LLM 的向量表示from importlib.metadata import versionimport reimport tiktokenimport osos.environ['HTTP_PROXY'] = 'http://l00841179:15350068547Aa%3F@proxyhk.huawei.com:8080'os.environ['HTTPS_PROXY'] = 'http://l00841179:15350068547Aa%3F@proxyhk.huawei.com:8080'# print("torch version:", version("torch"))# print("tiktoken version:", version("tiktoken"))# 读取文本with open("the-verdict.txt", "r", encoding="utf-8") as f:    raw_text = f.read()print("Total number of character:", len(raw_text))print(raw_text[:99])# 基本分词器应用preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)preprocessed = [item.strip() for item in preprocessed if item.strip()]print(len(preprocessed))# 创建词汇表all_words = sorted(set(preprocessed))vocab_size = len(all_words)print(vocab_size)vocab = {token:integer for integer,token in enumerate(all_words)}# 实现一个简单的文本tokenizerclass SimpleTokenizerV1:    def __init__(self, vocab):        self.str_to_int = vocab                                                   #A        self.int_to_str = {i:s for s,i in vocab.items()}                          #B    def encode(self, text):                                                       #C        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)        preprocessed = [item.strip() for item in preprocessed if item.strip()]        ids = [self.str_to_int[s] for s in preprocessed]        return ids    def decode(self, ids):                                                        #D        text = " ".join([self.int_to_str[i] for i in ids])        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)                           #E        return text#A 将词汇表作为类属性存储，以方便在 encode 和 decode 方法中访问#B 创建一个反向词汇表，将token ID 映射回原始的文本token#C 将输入文本转换为token ID#D 将token ID 还原为文本#E 在指定的标点符号前去掉空格tokenizer = SimpleTokenizerV1(vocab)text = """"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""ids = tokenizer.encode(text)print(ids)# 将两个特殊token 和 <|endoftext|> 包含在内all_tokens = sorted(list(set(preprocessed)))all_tokens.extend(["<|endoftext|>", "<|unk|>"])vocab = {token:integer for integer,token in enumerate(all_tokens)}# 实现一个简单的文本tokenizer：覆盖未包含单词和填充标记class SimpleTokenizerV2:    def __init__(self, vocab):        self.str_to_int = vocab        self.int_to_str = { i:s for s,i in vocab.items()}    def encode(self, text):        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)        preprocessed = [item.strip() for item in preprocessed if item.strip()]        preprocessed = [item if item in self.str_to_int                    #A                        else "<|unk|>" for item in preprocessed]        ids = [self.str_to_int[s] for s in preprocessed]        return ids    def decode(self, ids):        text = " ".join([self.int_to_str[i] for i in ids])        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)                    #B        return text#A 用 <|unk|> tokens替换未知词汇#B 在指定标点符号前替换空格text1 = "Hello, do you like tea?"text2 = "In the sunlit terraces of the palace."text = " <|endoftext|> ".join((text1, text2))tokenizer = SimpleTokenizerV2(vocab)print(tokenizer.encode(text))# 通过tiktoken实例化BPE分词器tokenizer = tiktoken.get_encoding('gpt2')text = "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace."integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})print(integers)strings = tokenizer.decode(integers)print(strings)text = "Akwirw ier"integers = tokenizer.encode(text)print(integers)strings = tokenizer.decode(integers)print(strings)