import torch


# A modified text generation function with more diversity
def generate(model, idx, max_new_tokens, context_size,
             temperature=1.0, top_k=None, eos_id=None):
    for _ in range(max_new_tokens):  # A
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]
        if top_k is not None:  # B
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(
                logits < min_val,
                torch.tensor(float('-inf')).to(logits.device),
                logits
            )

        if temperature > 0.0:  # C
            logits = logits / temperature
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        else:  # D
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)
        if idx_next == eos_id:  # E
            break
        idx = torch.cat((idx, idx_next), dim=1)
    return idx

# A For循环与之前相同：获取logits，仅关注最后的时间步
# B 在新步骤中，通过top-k采样过滤logits
# C 在新步骤中应用temperature scaling
# D 在未使用temperature scaling时，执行贪婪的下一个token选择
# E 如果遇到序列结束token且指定了eos_id，则提前停止生成
